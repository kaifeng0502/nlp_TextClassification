{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意(attention)！开始做前必读项！\n",
    "所有的代码一定要在这个文件里面编写，不要自己创建一个新的文件 对于提供的数据集，不要改存储地方，也不要修改文件名和内容 不要重新定义函数（如果我们已经定义好的），按照里面的思路来编写。当然，除了我们定义的部分，如有需要可以自行定义函数或者模块 写完之后，重新看一下哪一部分比较慢，然后试图去优化。一个好的习惯是每写一部分就思考这部分代码的时间复杂度和空间复杂度，AI工程是的日常习惯！ \n",
    "这次作业很重要，一定要完成！ 相信会有很多的收获！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练集文件和测试集文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text,data=[],[]\n",
    "stopWords=[]\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    函数说明：该函数用于加载数据集\n",
    "    return: \n",
    "        -data: 表示所有数据拼接的原始数据\n",
    "        -data_text: 表示数据集中的特征数据集\n",
    "        -datatext: 表示经过分词之后的特征数据集\n",
    "        -stopWords: 表示读取的停用词\n",
    "    '''\n",
    "    print('load Pre_process')\n",
    "    data = pd.concat([\n",
    "        pd.read_csv('data/train_clean.csv', sep='\\t'),\n",
    "        pd.read_csv('data/dev_clean.csv', sep='\\t'),\n",
    "        pd.read_csv('data/test_clean.csv', sep='\\t')\n",
    "        ])\n",
    "    print(\"读取数据集完成\")\n",
    "    data_text = list(data.text)  # .apply(lambda x: x.split(' '))\n",
    "    datatext = []\n",
    "    for i in range(len(data_text)):\n",
    "        datatext.append(data_text[i].split(' '))\n",
    "    stopWords = open('data/stopwords.txt').readlines()\n",
    "    print(\"取停用词完成\")\n",
    "    return data, data_text,datatext, stopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Pre_process\n",
      "读取数据集完成\n",
      "取停用词完成\n"
     ]
    }
   ],
   "source": [
    "data, data_text, datatext, stopWords=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    额吉 的 白云 优秀 蒙古文 文学作品 翻译 出版 工程 第五辑 散文 卷 散文集 额吉 的...\n",
       "1    佛罗伦萨 在 哪里 佛罗伦萨 在 哪里 题意 深沉 , 作者 置身于 佛罗伦萨 却 反复 叩...\n",
       "1    历代 茶 诗集 成宋 金卷 本书 主要 内容 包括 : 丁开 摘句 一首 、 丁带 茶 诗 ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_tfidf():\n",
    "    '''\n",
    "    函数说明：该函数用于训练tfidf的词向量\n",
    "    return: \n",
    "        -tfidf: 表示经过TF-ID模型训练出的词向量\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 根据数据集训练tfidf的词向量\n",
    "    # 第一步：首先通过TfidfVectorizer创建一个模型对象\n",
    "    count_vect = TfidfVectorizer(stop_words=stopWords,max_df =0.4,min_df=0.001,ngram_range=(1,2))\n",
    "    # 第二步：用模型对象去fit训练数据集\n",
    "    tfidf = count_vect.fit(data['text'])\n",
    "    print('train tfidf_embedding')\n",
    "    #返回是一个稀疏矩阵\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaifeng/ml/env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', 'sub', 'sup', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｃ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '一一', '一个', '一些', '一切', '一则', '一方面', '一旦', '一来', '一样', '一番', '一直', '一般', '万一', '上下', '不仅', '不但', '不光', '不单', '不只', '不如', '不怕', '不惟', '不成', '不拘', '不比', '不然', '不特', '不独', '不管', '不论', '不过', '不问', '与其', '与否', '与此同时', '两者', '为了', '为什么', '为何', '为着', '乃至', '之一', '之所以', '之类', '乌乎', '也好', '也就是说', '也罢', '于是', '于是乎', '云云', '人家', '什么', '什么样', '从而', '他人', '他们', '以便', '以免', '以及', '以至', '以至于', '以致', '任何', '任凭', '似的', '但是', '何况', '何处', '何时', '作为', '你们', '使得', '例如', '依照', '俺们', '倘使', '倘或', '倘然', '倘若', '假使', '假如', '假若', '关于', '其一', '其中', '其二', '其他', '其余', '其它', '其次', '具体地说', '具体说来', '再者', '再说', '况且', '几时', '凭借', '别的', '别说', '前后', '前者', '加之', '即令', '即使', '即便', '即或', '即若', '及其', '及至', '反之', '反过来', '反过来说', '另一方面', '另外', '只是', '只有', '只要', '只限', '叮咚', '可以', '可是', '可见', '各个', '各位', '各种', '各自', '同时', '向着', '否则', '吧哒', '呜呼', '呼哧', '咱们', '哈哈', '哎呀', '哎哟', '哪个', '哪些', '哪儿', '哪天', '哪年', '哪怕', '哪样', '哪边', '哪里', '哼唷', '啪达', '喔唷', '嗡嗡', '嘎登', '因为', '因此', '因而', '固然', '在下', '多少', '她们', '如上所述', '如何', '如其', '如果', '如此', '如若', '宁可', '宁愿', '宁肯', '它们', '对于', '尔后', '尚且', '就是', '就是说', '尽管', '岂但', '并且', '开外', '开始', '当着', '彼此', '怎么', '怎么办', '怎么样', '怎样', '总之', '总的来看', '总的来说', '总的说来', '总而言之', '恰恰相反', '慢说', '我们', '或是', '或者', '所以', '抑或', '按照', '换句话说', '换言之', '接着', '故此', '旁人', '无宁', '无论', '既是', '既然', '时候', '是的', '有些', '有关', '有的', '有的是', '朝着', '本着', '来着', '极了', '果然', '果真', '某个', '某些', '根据', '正如', '此外', '此间', '毋宁', '每当', '比如', '比方', '沿着', '漫说', '然则', '然后', '然而', '照着', '甚么', '甚而', '甚至', '由于', '由此可见', '的话', '相对而言', '省得', '着呢', '第二', '等等', '紧接着', '纵令', '纵使', '纵然', '经过', '结果', '继而', '综上所述', '罢了', '而且', '而况', '而外', '而已', '而是', '而言', '自个儿', '自从', '自各儿', '自家', '自己', '自身', '至于', '若是', '若非', '莫若', '虽则', '虽然', '虽说', '要不', '要不是', '要不然', '要么', '要是', '许多', '设使', '设若', '诸位', '谁知', '起见', '趁着', '越是', '较之', '还是', '还有', '这个', '这么', '这么些', '这么样', '这么点儿', '这些', '这会儿', '这儿', '这就是说', '这时', '这样', '这边', '这里', '进而', '连同', '通过', '遵照', '那个', '那么', '那么些', '那么样', '那些', '那会儿', '那儿', '那时', '那样', '那边', '那里', '鄙人', '鉴于', '除了', '除此之外', '除非', '随着', '非但', '非徒', '顺着', '首先', '１２', 'ｌｉ', 'ｎｇ昉', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tfidf_embedding\n"
     ]
    }
   ],
   "source": [
    "tfidf = trainer_tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据数据集训练word2vec的词向量  \n",
    "利用models.Word2Vec训练w2v的词向量  \n",
    "部分参数说明：  \n",
    "min_count：表示低于该频率的词将会删除，  \n",
    "window：表示滑动窗口大小，  \n",
    "alpha：表示学习率，  \n",
    "negative：表示负采样样本个数，  \n",
    "max_vocab_size：表示RAM中最大限制的词个数  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_w2v():\n",
    "    '''\n",
    "    函数说明：该函数基于Word2vec模型训练词向量\n",
    "    return: \n",
    "        -w2v: 表示经过word2vec模型训练出的词向量\n",
    "    '''\n",
    "    print('train word2vec Embedding')\n",
    "    # 训练w2v的词向量\n",
    "    # 第一步：利用models.Word2Vec构建一个模型，\n",
    "    w2v = models.Word2Vec(min_count=2,\n",
    "                                window=3,\n",
    "                                size=300,\n",
    "                                sample=6e-5,\n",
    "                                alpha=0.03,\n",
    "                                min_alpha=0.0007,\n",
    "                                negative=15,\n",
    "                                workers=4,\n",
    "                                iter=10,\n",
    "                                max_vocab_size=50000) \n",
    "        \n",
    "    w2v.build_vocab(datatext)\n",
    "\n",
    "    w2v.train(datatext,\n",
    "              total_examples=w2v.corpus_count,\n",
    "              epochs=15,\n",
    "              report_delay=1)\n",
    "        \n",
    "    \n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word2vec Embedding\n",
      "train fast_embedding\n"
     ]
    }
   ],
   "source": [
    "w2v = trainer_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.FastText?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_fasttext():\n",
    "    '''\n",
    "    函数说明：该函数基于FastText模型训练词向量\n",
    "    return: \n",
    "        -fast: 表示经过FastText模型训练出的词向量\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 根据数据集训练FastTExt的词向量\n",
    "    # hint: 利用models.FastText,\n",
    "    # 可使用部分参数说明：\n",
    "    # size：生成的向量维度，\n",
    "    # window: 移动窗口，\n",
    "    # aphla: 学习率，\n",
    "    # min_count: 对低于该频率的词进行截断\n",
    "    # 可以参照trainer_w2v函数完成FastText的词向量的训练\n",
    "    # 可以直接根据models.FastText直接训练样本获取词向量\n",
    "    fast = models.FastText(datatext,size=300,window=3,alpha=0.03,min_count=2,iter=30,word_ngrams=2,max_vocab_size=50000)\n",
    "    return fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast= trainer_fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save tfidf model\n",
      "save word2vec model\n",
      "save fast model\n"
     ]
    }
   ],
   "source": [
    "def saver():\n",
    "    '''\n",
    "    函数说明：该函数存储训练好的模型\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    print('save tfidf model')\n",
    "    joblib.dump(tfidf,'tfidf')\n",
    "\n",
    "    # hint: w2v可以通过自带的save函数进行保存\n",
    "    print('save word2vec model')\n",
    "    w2v.wv.save_word2vec_format('w2v.bin',binary=False)\n",
    "     # hint: fast可以通过自带的save函数进行保存\n",
    "    print('save fast model')\n",
    "    fast.wv.save_word2vec_format('fast.bin',binary=False)\n",
    "\n",
    "    \n",
    "saver()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tfidf_embedding model\n",
      "load w2v_embedding model\n",
      "load fast_embedding model\n"
     ]
    }
   ],
   "source": [
    "def load():\n",
    "    '''\n",
    "    函数说明：该函数加载训练好的模型\n",
    "    '''\n",
    "    #ToDo\n",
    "    # 加载模型 \n",
    "    # hint: tfidf可以通过joblib.load进行加载\n",
    "    # w2v和fast可以通过gensim.models.KeyedVectors.load加载\n",
    "    print('load tfidf_embedding model')\n",
    "    tfidf =  joblib.load('tfidf')\n",
    "    print('load w2v_embedding model')\n",
    "    w2v =  models.KeyedVectors.load_word2vec_format('w2v.bin',binary=False)\n",
    "    print('load fast_embedding model')\n",
    "    fast =  models.KeyedVectors.load_word2vec_format('fast.bin',binary=False)\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
